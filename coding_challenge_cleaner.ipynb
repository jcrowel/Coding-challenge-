{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import telemetry_pb2\n",
    "import time\n",
    "import zmq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "#standard port for postgresql \n",
    "port = '5432'\n",
    "#name of my data base\n",
    "database = 'coding_challenge'\n",
    "user = 'postgres'\n",
    "#can't change my password... awk \n",
    "password = 'Jasper2020Ja!'\n",
    "\n",
    "# Create connection string\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying that i can access the database\n",
    "# basic practice query\n",
    "practice_query = 'SELECT * FROM missile_tracks_sensor_1;'\n",
    "\n",
    "# Load query result into a DataFrame\n",
    "df = pd.read_sql_query(practice_query, engine)\n",
    "\n",
    "# Displaying the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Section of code is written to validate raw data by:\n",
    "    Identifying null/ missing values\n",
    "    Flagging values that are out of bounds in Latitude, Longitude, and Altitude\n",
    "    Flagging radiometric intensity outliers\n",
    "    and finally saving the summary to a .json for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating my own function to run through all the step to loop through later\n",
    "#my broze layer validation function \n",
    "def bronze_layer(sensor_name, engine, output_dir=\"reports\"):\n",
    "\n",
    "    \n",
    "    # Pulling in the data\n",
    "    query = f\"SELECT * FROM {sensor_name}\"\n",
    "    df = pd.read_sql(query, con=engine)\n",
    "    \n",
    "    #Counting the Nulls\n",
    "    null_counts = df.isnull().sum().to_dict()\n",
    "    #Identifying the rows that have nulls-- not the whole rows just the row numbers\n",
    "    null_row_indices = df[df.isnull().any(axis=1)].index.tolist()\n",
    "    \n",
    "    \n",
    "    #  Checking the lats, lons and altitude to make sure they are in range.\n",
    "    #Altitude for missiles can't be below sea level \n",
    "    out_of_bounds = df[\n",
    "        (df[\"latitude\"] <= -90) | (df[\"latitude\"] >= 90) |\n",
    "        (df[\"longitude\"] <= -180) | (df[\"longitude\"] >= 180) |\n",
    "        (df[\"altitude\"] < 0)\n",
    "    ]\n",
    "    \n",
    "    #Identifying the rows that are out of bounds-- not the whole rows just the row numbers\n",
    "    out_of_bounds_indices = out_of_bounds.index.tolist()\n",
    "    \n",
    "    # Flagging the Radiometric outliers (IQR) \n",
    "    # Chatgpt helped with these calculaitons \n",
    "    #these calculations are the middle 50% of the gaussian distribution\n",
    "    q1 = df[\"radiometric_intensity\"].quantile(0.25)\n",
    "    q3 = df[\"radiometric_intensity\"].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    \n",
    "    outliers = df[\n",
    "        (df[\"radiometric_intensity\"] < lower) |\n",
    "        (df[\"radiometric_intensity\"] > upper)\n",
    "    ]\n",
    "    #identifying outlier rows -- not the whole rows just the row numbers\n",
    "    outlier_indices = outliers.index.tolist()\n",
    "    \n",
    "    # Building my summary dictionary \n",
    "    #to be saved in the json in this order with these labels\n",
    "    report = {\n",
    "        \"sensor\": sensor_name,\n",
    "        \"row_count\": len(df),\n",
    "        \"null_counts\": null_counts,\n",
    "        \"null_row_indices\": null_row_indices,\n",
    "        \"position_out_of_bounds_count\": len(out_of_bounds),\n",
    "        \"position_out_of_bounds_indices\": out_of_bounds_indices,\n",
    "        \"radiometric_outlier_count\": len(outliers),\n",
    "        \"radiometric_outlier_indices\": outlier_indices\n",
    "    }\n",
    "\n",
    "    #Saving the summary as JSON\n",
    "    #and saving it to a reports directory with the sensor name +_validation\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    out_path = Path(output_dir) / f\"{sensor_name}_validation.json\"\n",
    "    \n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\" Report saved: {out_path}\")\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a for loop to go through each missile track table \n",
    "#this runs it through the function defined above\n",
    "\n",
    "for i in range(1, 6):\n",
    "    bronze_layer(f\"missile_tracks_sensor_{i}\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # This section of code is written to clean and enrich the data by:\n",
    "    Interpolating small gaps in data, \n",
    "    Excluding larger gaps from the data, \n",
    "    Smoothing the radiometric intensity data to reduce noise, \n",
    "    and finally saving the cleaned data in a .json file for future use, \n",
    "    \n",
    "# Trade-offs\n",
    "\n",
    "| Method        | Pros                          | Cons                         |\n",
    "|---------------|-------------------------------|------------------------------|\n",
    "| Interpolation | Fills in the minor gaps       | This can hide real anomalies |\n",
    "| Exclusion     | Avoids bad data               | Reduces the amount of data   |\n",
    "| Smoothing     | Smooths noise                 | Can distort the true nature  |\n",
    "\n",
    "###  Visual Confirmation\n",
    "See plot below showing radiometric intensity before/after smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_layer(sensor_name, engine, gap_threshold=5, smooth_window=10, output_dir=\"clean\"):\n",
    "        \n",
    "    # Reading in the table of data\n",
    "    df = pd.read_sql(f\"SELECT * FROM {sensor_name}\", con=engine)\n",
    "    \n",
    "    #Sorting data by time\n",
    "    #creating a data frame to log the change (delta) in time (t) of the unix timestamp\n",
    "    #using .fillna(0) to fill in the value above the first with 0\n",
    "    df = df.sort_values(\"unix_timestamp\")\n",
    "    df[\"delta_t\"] = df[\"unix_timestamp\"].diff().fillna(0)\n",
    "    \n",
    "    #Creating a table/data frame to log the gaps that are greater than 5 seconds .\n",
    "    df[\"time_gap_flag\"] = df[\"delta_t\"] > 5  \n",
    "    \n",
    "    #Interpolating the smaller gaps\n",
    "    #making a copy of the table to use for interpolation \n",
    "    interpolated = df.copy()\n",
    "    \n",
    "    # Identifying the small gaps (less then 5 seconds)to be interpolated, \n",
    "    #the columns that can be interpolated, and then the rows that need to be interpolated\n",
    "    interpolated = df.copy()\n",
    "    columns_to_interp = [\"latitude\", \"longitude\", \"altitude\", \"radiometric_intensity\"]\n",
    "    safe_rows = ~df[\"time_gap_flag\"]\n",
    "    \n",
    "    #Locating the rows within the columns that need to be interoplated and then interpolating them linearly \n",
    "    interpolated.loc[safe_rows, columns_to_interp] = df.loc[safe_rows, columns_to_interp].interpolate(method=\"linear\")\n",
    "\n",
    "    #Excluding the rows with large gaps\n",
    "    cleaned = interpolated[~df[\"time_gap_flag\"]].copy().reset_index(drop=True)\n",
    "\n",
    "    # Smoothing radiometric intensity with a rolling window of 10 data points (identified in the function parameters)\n",
    "    cleaned[\"radiometric_smoothed\"] = cleaned[\"radiometric_intensity\"].rolling(window=smooth_window, center=True).mean()\n",
    "\n",
    "    # Ploting the radiometric intensity smooth v original\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df[\"unix_timestamp\"], df[\"radiometric_intensity\"], label=\"Original\", alpha=0.4)\n",
    "    plt.plot(cleaned[\"unix_timestamp\"], cleaned[\"radiometric_smoothed\"], label=\"Smoothed\", linewidth=2)\n",
    "    plt.title(\"Radiometric Intensity: Original vs. Smooth \")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Intensity\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "  \n",
    "    \n",
    "    #  Saving to a .json\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    out_path = Path(output_dir) / f\"{sensor_name}_cleaned.json\"\n",
    "    #saving the output in a more table layout or readablity \n",
    "    cleaned.to_json(f\"clean/{sensor_name}_cleaned.json\", orient=\"records\", indent=2)\n",
    "    \n",
    "    # Adding the cleaned data to the Database in pgadmin4\n",
    "    table_name = \"cleaned_telemetry\"\n",
    "    try:\n",
    "        cleaned.to_sql(table_name, engine, if_exists=\"append\", index=False)\n",
    "        print(f\" {sensor_name} saved to PostgreSQL table '{table_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {sensor_name} to PostgreSQL: {e}\")\n",
    "\n",
    "    print(f\"Cleaned JSON saved: {out_path}\")\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    silver_layer(f\"missile_tracks_sensor_{i}\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section of code was written to serialize and stream data:\n",
    "    Serializes telemetry data row-by-row using Protobuf serialization \n",
    "    Then streams the data using ZeroMQ streaming.\n",
    "        - If there is an error it will skip that row, but this could result in loss of data \n",
    "        - If the connection fails that error will be cought and printed\n",
    "    \n",
    "    I have set up a seperate notebook to simulate streaming between two sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating my own function to serialize and stream data \n",
    "def serialize_stream(cleaned_df, zmq_port=5555, max_delay=1.0, log_name=\"stream\"):\n",
    "    \n",
    "    #setting up ZeroMQ \n",
    "    #I am not familiar with ZeroMQ so chatgpt created this\n",
    "    try:\n",
    "        context = zmq.Context()\n",
    "        socket = context.socket(zmq.PUSH)\n",
    "        socket.bind(f\"tcp://127.0.0.1:{zmq_port}\")  \n",
    "        #printing a start message\n",
    "        print(f\" Streaming to ZeroMQ socket on port {zmq_port}...\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not connect to port {zmq_port}: {e}\")\n",
    "        return  \n",
    "    \n",
    "    #Sorting data by time \n",
    "    cleaned_df = cleaned_df.sort_values(\"unix_timestamp\").reset_index(drop=True)\n",
    "    n = len(cleaned_df)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #loop throw each row of data\n",
    "    for i in range(n):\n",
    "        row = cleaned_df.iloc[i]\n",
    "\n",
    "        # Create Protobuf message\n",
    "        #I've never used protobufs so chatgpt created this \n",
    "        msg = telemetry_pb2.TelemetryData(\n",
    "            unix_timestamp=int(row[\"unix_timestamp\"]),\n",
    "            latitude=float(row[\"latitude\"]),\n",
    "            longitude=float(row[\"longitude\"]),\n",
    "            altitude=float(row[\"altitude\"]),\n",
    "            radiometric_intensity=float(row[\"radiometric_intensity\"]),\n",
    "            radiometric_smoothed=float(row[\"radiometric_smoothed\"])\n",
    "        )\n",
    "\n",
    "        # Turning the message defined above into binary\n",
    "        try:\n",
    "            binary_data = msg.SerializeToString()\n",
    "            socket.send(binary_data)\n",
    "            print(f\"[{i}] Sent message at timestamp {row['unix_timestamp']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to serialize or send message at index {i}: {e}\")\n",
    "            continue  # Skip this row and keep streaming\n",
    "        \n",
    "        #simulates the sending of data based on the times \n",
    "        if i < len(cleaned_df) - 1:\n",
    "            delta = cleaned_df.iloc[i + 1][\"unix_timestamp\"] - row[\"unix_timestamp\"]\n",
    "            time.sleep(min(delta, max_delay))\n",
    "            \n",
    "            \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        throughput = n / duration\n",
    "        latency = duration / n\n",
    "    \n",
    "    \n",
    "    print(\"\\n Stream complete.\")\n",
    "    print(f\"Throughput: {throughput:.2f} messages/sec\")\n",
    "    print(f\" Average latency per message: {latency:.6f} sec\")\n",
    "\n",
    "       \n",
    "    socket.close()\n",
    "    context.term()   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    sensor = f\"missile_tracks_sensor_{i}\"\n",
    "    cleaned = pd.read_json(f\"clean/{sensor}_cleaned.json\")\n",
    "    serialize_stream(cleaned, zmq_port=5555 + i, log_name=sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency and Performance Metrics\n",
    "\n",
    "#### Testing Environment:\n",
    "\n",
    "    Machine: MacBook Pro 3.1 GHz Dual-Core Intel Core i5\n",
    "    Memory: 8 GB 2133 MHz LPDDR3\n",
    "    Python: 3, running in Jupyter Notebook\n",
    "    Streaming method: ZeroMQ \n",
    "    Serialization: Google Protobuf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Used a serialize_stream() function to stream telemetry data from one cleaned sensor file:\n",
    "    Timed 1 sensor to compute: ~12 min\n",
    "    Total stream time:\n",
    "    Average throughput (messages per second):242.60\n",
    "    Average latency per message: 0.004122 sec\n",
    "\n",
    "#### Optimizations Used:\n",
    "\n",
    "    Protobuf for fast serialization\n",
    "    ZeroMQ for message transport\n",
    "    No intermediate files used in streaming\n",
    "    PostgreSQL writes using to_sql(..., if_exists=\"append\")\n",
    "    Streaming runs in a loop with no redundant computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of code is almost the same as the above but is being used to test how the above can handle bust data.\n",
    "#for ease I made this chunk to be run with out any of the other code\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import telemetry_pb2\n",
    "import time\n",
    "import zmq\n",
    "\n",
    "\n",
    "def simulate_burst(cleaned_df, repeat=10, zmq_port=5555):\n",
    "    \n",
    "\n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.PUSH)\n",
    "    socket.bind(f\"tcp://127.0.0.1:{zmq_port}\")\n",
    "\n",
    "    total_messages = len(cleaned_df) * repeat\n",
    "    start = time.time()\n",
    "\n",
    "    for r in range(repeat):\n",
    "        for i, row in cleaned_df.iterrows():\n",
    "            try:\n",
    "                msg = telemetry_pb2.TelemetryData(\n",
    "                    unix_timestamp=int(row[\"unix_timestamp\"]),\n",
    "                    latitude=float(row[\"latitude\"]),\n",
    "                    longitude=float(row[\"longitude\"]),\n",
    "                    altitude=float(row[\"altitude\"]),\n",
    "                    radiometric_intensity=float(row[\"radiometric_intensity\"]),\n",
    "                    radiometric_smoothed=float(row[\"radiometric_smoothed\"])\n",
    "                )\n",
    "                socket.send(msg.SerializeToString())\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Row {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    throughput = total_messages / duration\n",
    "    latency = duration / total_messages\n",
    "\n",
    "    print(f\"Simulated {total_messages} messages in {duration:.2f} seconds\")\n",
    "    print(f\"Throughput: {throughput:.2f} msgs/sec\")\n",
    "    print(f\"Average latency: {latency:.6f} sec\")\n",
    "\n",
    "    socket.close()\n",
    "    context.term()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated 3536240 messages in 680.78 seconds\n",
      "Throughput: 5194.39 msgs/sec\n",
      "Average latency: 0.000193 sec\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"clean/missile_tracks_sensor_1_cleaned.json\")\n",
    "simulate_burst(df, repeat=20, zmq_port=5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability Demonstration: Bust scenario\n",
    "\n",
    "To test whether or not this script can handle burst data, a for loop is used to quickly run and send data repeatedly, sending thousands of messages to a ZeroMQ stream in memory.\n",
    "\n",
    "\n",
    "\n",
    "#### Testing Environment:\n",
    "    simulate_burst() function sent 3,000,000 messages in a loop\n",
    "    \n",
    "    Machine: MacBook Pro 3.1 GHz Dual-Core Intel Core i5\n",
    "    Memory: 8 GB 2133 MHz LPDDR3\n",
    "    Python: 3, running in Jupyter Notebook\n",
    "    Streaming method: ZeroMQ \n",
    "    Serialization: Google Protobuf\n",
    "\n",
    "#### Results:\n",
    "- Total messages: 3,536,240\n",
    "- Duration: 680.78 seconds\n",
    "- Throughput:~ 5194.39 messages/second\n",
    "- Latency: ~0.000193 seconds per message\n",
    "\n",
    "These results confirm that the pipeline can sustain high message volumes on very modest hardware, and would do even better with multiprocessing or distributed receivers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
